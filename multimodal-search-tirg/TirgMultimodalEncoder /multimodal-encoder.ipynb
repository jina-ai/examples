{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, sys\n",
    "from tqdm import tqdm\n",
    "import torch, torchvision\n",
    "\n",
    "from datasets import Fashion200k\n",
    "from img_text_composition_models import TIRG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 512\n",
    "dataset_path = '/Users/bo/Downloads/200k/'\n",
    "model_path = '/Users/bo/Downloads/checkpoint_fashion200k.pth'\n",
    "text_path = '/Users/bo/Downloads/texts.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read pants_train_detect_all.txt\n",
      "read dress_train_detect_all.txt\n",
      "read jacket_train_detect_all.txt\n",
      "read skirt_train_detect_all.txt\n",
      "read top_train_detect_all.txt\n",
      "Fashion200k: 172049 images\n",
      "53099 unique cations\n",
      "Modifiable images 106464\n",
      "read top_test_detect_all.txt\n",
      "read pants_test_detect_all.txt\n",
      "read dress_test_detect_all.txt\n",
      "read skirt_test_detect_all.txt\n",
      "read jacket_test_detect_all.txt\n",
      "Fashion200k: 29789 images\n"
     ]
    }
   ],
   "source": [
    "trainset = Fashion200k(path=dataset_path, \n",
    "                       split='train',        \n",
    "                        transform=torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.Resize(224),\n",
    "                        torchvision.transforms.CenterCrop(224),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])\n",
    "                    ]))\n",
    "\n",
    "testset = Fashion200k(path=dataset_path, \n",
    "                       split='test',        \n",
    "                        transform=torchvision.transforms.Compose([\n",
    "                        torchvision.transforms.Resize(224),\n",
    "                        torchvision.transforms.CenterCrop(224),\n",
    "                        torchvision.transforms.ToTensor(),\n",
    "                        torchvision.transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                                         [0.229, 0.224, 0.225])\n",
    "                    ]))\n",
    "\n",
    "texts =  trainset.get_all_texts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "__copyright__ = \"Copyright (c) 2020 Jina AI Limited. All rights reserved.\"\n",
    "__license__ = \"Apache-2.0\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from jina.executors.devices import TorchDevice\n",
    "from jina.excepts import PretrainedModelFileDoesNotExist\n",
    "from jina.executors.decorators import batching_multi_input, as_ndarray\n",
    "from jina.executors.encoders.multimodal import BaseMultiModalEncoder\n",
    "\n",
    "# sys.path.append(\".\")\n",
    "from img_text_composition_models import TIRG\n",
    "\n",
    "class TirgMultiModalEncoder(TorchDevice, BaseMultiModalEncoder):\n",
    "\n",
    "    def __init__(self, model_path: str,\n",
    "                 texts_path: str,\n",
    "                 positional_modality: List[str] = ['visual', 'textual'],\n",
    "                 channel_axis: int = -1, \n",
    "                 *args, **kwargs):\n",
    "        \"\"\"\n",
    "        :param model_path: the path where the model is stored.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.model_path = model_path\n",
    "        self.texts_path = texts_path\n",
    "        self.positional_modality = positional_modality\n",
    "        self.channel_axis = channel_axis\n",
    "        # axis 0 is the batch\n",
    "        self._default_channel_axis = 1\n",
    "\n",
    "    def post_init(self):\n",
    "        super().post_init()\n",
    "        import torch\n",
    "        if self.model_path and os.path.exists(self.model_path):\n",
    "            with open (self.texts_path, 'rb') as fp:\n",
    "                texts = pickle.load(fp)\n",
    "            self.model = TIRG(texts, 512)\n",
    "            model_sd = torch.load(self.model_path, map_location=torch.device('cpu'))\n",
    "            self.model.load_state_dict(model_sd['model_state_dict'])\n",
    "            self.model.eval()\n",
    "            self.to_device(self.model)\n",
    "        else:\n",
    "            raise PretrainedModelFileDoesNotExist(f'model {self.model_path} does not exist')\n",
    "\n",
    "    def _get_features(self, data):\n",
    "        visual_data = data[(self.positional_modality.index('image'))]\n",
    "        if self.channel_axis != self._default_channel_axis:\n",
    "            visual_data = np.moveaxis(visual_data, self.channel_axis, self._default_channel_axis)\n",
    "        textual_data = data[(self.positional_modality.index('text'))]\n",
    "        \n",
    "        visual_data = torch.stack(visual_data).float()\n",
    "\n",
    "        if self.on_gpu:\n",
    "            visual_data = visual_data.cuda()\n",
    "            textual_data = textual_data.cuda()\n",
    "            \n",
    "        img_features = self.model.extract_img_feature(visual_data)\n",
    "        text_features = self.model.extract_text_feature(textual_data)\n",
    "        \n",
    "        return self.model.compose_img_text_features(img_features, text_features)\n",
    "\n",
    "    @batching_multi_input\n",
    "    @as_ndarray\n",
    "    def encode(self, *data: 'np.ndarray', **kwargs) -> 'np.ndarray':\n",
    "        import torch\n",
    "        feature = self._get_features(*data).detach()\n",
    "        if self.on_gpu:\n",
    "            feature = feature.cpu()\n",
    "        feature = feature.numpy()\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TirgMultiModalEncoder@43638[I]:\u001b[37mpost initiating, this may take some time...\u001b[0m\n",
      "TirgMultiModalEncoder@43638[I]:\u001b[37mpost initiating, this may take some time takes 1 second (1.87s)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "encoder = TirgMultiModalEncoder(model_path, text_path, positional_modality = ['image', 'text'], channel_axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example, encode one single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "data.append([trainset.get_img(0)]) # image at position 0\n",
    "data.append([texts[0]]) # text at position 1\n",
    "encoded_multimodal = encoder.encode(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_multimodal.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode a batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "fashion_200k_loader = trainset.get_loader(batch_size=batch_size)\n",
    "for batch in fashion_200k_loader:\n",
    "    # use multimodal encoder\n",
    "    data = []\n",
    "    assert len(batch) == batch_size\n",
    "    batch_of_imgs = [item['source_img_data'] for item in batch]\n",
    "    batch_of_text = [item['source_caption'] for item in batch]\n",
    "    data.append(batch_of_imgs)\n",
    "    data.append(batch_of_text)\n",
    "    assert len(data) == 2\n",
    "    encoded_batch = encoder.encode(data)\n",
    "    assert len(encoded_batch) == batch_size\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "1. Ensure encoder works the same as the original model at instance level\n",
    "2. Ensure encoder works the same as the original model at batch level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = TIRG(texts, embed_dim)\n",
    "model_sd = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "model.load_state_dict(model_sd['model_state_dict'])\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing multimodal encoder with img 0\n",
      "testing multimodal encoder with img 1\n",
      "testing multimodal encoder with img 2\n",
      "testing multimodal encoder with img 3\n",
      "testing multimodal encoder with img 4\n",
      "testing multimodal encoder with img 5\n",
      "testing multimodal encoder with img 6\n",
      "testing multimodal encoder with img 7\n",
      "testing multimodal encoder with img 8\n",
      "testing multimodal encoder with img 9\n"
     ]
    }
   ],
   "source": [
    "# Ensure encoded result is correct at instance level\n",
    "for i in range(10):\n",
    "    print(f\"testing multimodal encoder with img {i}\")\n",
    "    # extract feature via jina encoder\n",
    "    data = []\n",
    "    data.append([trainset.get_img(i)]) # visual at position 0\n",
    "    data.append([texts[i]]) # textual at position 1\n",
    "    encoded = encoder.encode(data)\n",
    "    # extract image text feature\n",
    "    text_feature = model.extract_text_feature([texts[i]])\n",
    "    img = [trainset.get_img(i)]\n",
    "    img = torch.stack(img).float()\n",
    "    img_feature =  model.extract_img_feature(img)\n",
    "    extracted = model.compose_img_text_features(img_feature, text_feature).cpu().detach().numpy()\n",
    "    assert encoded.shape == extracted.shape\n",
    "    assert encoded.all() == extracted.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2689 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing multimodal encoder with batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2689 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "# Ensure encoded result is correct at batch level\n",
    "batch_size=64\n",
    "fashion_200k_loader = trainset.get_loader(batch_size=batch_size)\n",
    "for batch in tqdm(fashion_200k_loader):\n",
    "    print(f\"testing multimodal encoder with batch size {batch_size}\")\n",
    "    # use multimodal encoder\n",
    "    data = []\n",
    "    assert len(batch) == batch_size\n",
    "    batch_of_imgs = [item['source_img_data'] for item in batch]\n",
    "    batch_of_text = [item['source_caption'] for item in batch]\n",
    "    data.append(batch_of_imgs)\n",
    "    data.append(batch_of_text)\n",
    "    assert len(data) == 2\n",
    "    encoded_batch = encoder.encode(data)\n",
    "    # use the original model\n",
    "    batch_of_text_features = model.extract_text_feature(batch_of_text)\n",
    "    batch_of_imgs = torch.stack(batch_of_imgs).float()\n",
    "    batch_of_img_features =  model.extract_img_feature(batch_of_imgs)\n",
    "    extracted_batch = model.compose_img_text_features(batch_of_img_features, batch_of_text_features).cpu().detach().numpy()\n",
    "    assert len(extracted_batch) == batch_size\n",
    "    assert extracted_batch.all() == encoded_batch.all()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
